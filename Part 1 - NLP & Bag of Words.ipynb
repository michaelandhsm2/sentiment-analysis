{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# First Try - Natural Language Processing with Bag of Words\n",
    "\n",
    "\n",
    "Using Libaries:\n",
    "*   Pandas  -  .\n",
    "*   Beautiful Soup  -  .\n",
    "*   Re  -  .\n",
    "*   numpy  -  .\n",
    "*   CountVectorizer  -  .\n",
    "*   RandomForestClassifier  -  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearing Up the Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the pandas package, then use the \"read_csv\" function to read the labeled training data\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"./data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the read data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column header value\n",
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It must be assumed that those who praised this film (\\\"the greatest filmed opera ever,\\\" didn't I read somewhere?) either don't care for opera, don't care for Wagner, or don't care about anything except their desire to appear Cultured. Either as a representation of Wagner's swan-song, or as a movie, this strikes me as an unmitigated disaster, with a leaden reading of the score matched to a tricksy, lugubrious realisation of the text.<br /><br />It's questionable that people with ideas as to wha......\n"
     ]
    }
   ],
   "source": [
    "# Sample review output\n",
    "print(train[\"review\"][3][0:500]+\"......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is filled with messy HTML tags, i.e. &lt;br/&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It must be assumed that those who praised this film (\\\"the greatest filmed opera ever,\\\" didn't I read somewhere?) either don't care for opera, don't care for Wagner, or don't care about anything except their desire to appear Cultured. Either as a representation of Wagner's swan-song, or as a movie, this strikes me as an unmitigated disaster, with a leaden reading of the score matched to a tricksy, lugubrious realisation of the text.It's questionable that people with ideas as to what an opera (......\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# Fix Output\n",
    "from bs4 import BeautifulSoup\n",
    "example1 = BeautifulSoup(train[\"review\"][3])  \n",
    "print(example1.get_text()[0:500]+\"......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first try, we're removing all the numbers & punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It must be assumed that those who praised this film the greatest filmed opera ever didnt I read somewhere either dont care for opera dont care for Wagner or dont care about anything except their desire to appear Cultured Either as a representation of Wagners swansong or as a movie this strikes me as an unmitigated disaster with a leaden reading of the score matched to a tricksy lugubrious realisation of the textIts questionable that people with ideas as to what an opera or for that matter a play......\n"
     ]
    }
   ],
   "source": [
    "#Removing everything but letters.\n",
    "import re\n",
    "letters_only = re.sub(\"[^a-zA-Z ]\",           # The pattern to search for\n",
    "                      \"\",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "print(letters_only[0:500]+\"......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to preform '*tokenization*' - basically moving all the words into a lowered form & splitting them into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "lower_case = letters_only.lower()       \n",
    "words = lower_case.split()              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to remove stop words - words that are commonly used without meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him']\n"
     ]
    }
   ],
   "source": [
    "# Showing stop words\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print(stopwords.words(\"english\")[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "must assumed praised film greatest filmed opera ever didnt read somewhere either dont care opera dont care wagner dont care anything except desire appear cultured either representation wagners swansong movie strikes unmitigated disaster leaden reading score matched tricksy lugubrious realisation textits questionable people ideas opera matter play especially one shakespeare allowed anywhere near theatre film studio syberberg fashionably without smallest justification wagners text decided parsifal......\n"
     ]
    }
   ],
   "source": [
    "# Using downloaded text data sets to remove stop words\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print (\" \".join( words)[0:500]+\"......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final result we want. <br/>\n",
    "Now, we put all these steps into one single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining new Function\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z ]\", \"\", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. List to Set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "must assumed praised film greatest filmed opera ever didnt read somewhere either dont care opera dont care wagner dont care anything except desire appear cultured either representation wagners swansong movie strikes unmitigated disaster leaden reading score matched tricksy lugubrious realisation textits questionable people ideas opera matter play especially one shakespeare allowed anywhere near theatre film studio syberberg fashionably without smallest justification wagners text decided parsifal......\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# Testing Function\n",
    "print(review_to_words(train[\"review\"][3])[0:500]+\"......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000;  Review 2000 of 25000;  Review 3000 of 25000;  Review 4000 of 25000;  Review 5000 of 25000;  Review 6000 of 25000;  Review 7000 of 25000;  Review 8000 of 25000;  Review 9000 of 25000;  Review 10000 of 25000;  Review 11000 of 25000;  Review 12000 of 25000;  Review 13000 of 25000;  Review 14000 of 25000;  Review 15000 of 25000;  Review 16000 of 25000;  Review 17000 of 25000;  Review 18000 of 25000;  Review 19000 of 25000;  Review 20000 of 25000;  Review 21000 of 25000;  Review 22000 of 25000;  Review 23000 of 25000;  Review 24000 of 25000;  Review 25000 of 25000;  \n",
      "\n",
      "must assumed praised film greatest filmed opera ever didnt read somewhere either dont care opera dont care wagner dont care anything except desire appear cultured either representation wagners swansong movie strikes unmitigated disaster leaden reading score matched tricksy lugubrious realisation textits questionable people ideas opera matter play especially one shakespeare allowed anywhere near theatre film studio syberberg fashionably without smallest justification wagners text decided parsifal......\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# Fix all reviews\n",
    "num_reviews = train[\"review\"].size\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Review %d of %d; \" % ( i+1, num_reviews ), \"\", end=\"\")            \n",
    "    clean_train_reviews.append(review_to_words(train[\"review\"][i]))\n",
    "\n",
    "print(\"\\n\\n\"+clean_train_reviews[3][0:500]+\"......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Bag of Words\n",
    "\n",
    "Using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a> from sklearn, we can count and find the most used tokens (or words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bag of Words using CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\\\n",
    "                             stop_words = None, max_features = 5000) \n",
    "\n",
    "# Fit Transform changes the vector into feature vectors.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews).toarray()\n",
    "\n",
    "# Check result\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'absence', 'absolute', 'absolutely', 'absurd', 'abuse', 'abused', 'abusive', 'abysmal', 'academy', 'accent', 'accents', 'accept', 'acceptable', 'accepted', 'access', 'accident', 'accidentally', 'accompanied', 'accomplished', 'according', 'account', 'accuracy', 'accurate', 'accused']\n"
     ]
    }
   ],
   "source": [
    "# Show Vocabs\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab[0:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( abandoned ,  184 ) ( abc ,  105 ) ( abilities ,  105 ) ( ability ,  445 ) ( able ,  1240 ) ( abraham ,  79 ) ( absence ,  115 ) ( absolute ,  347 ) ( absolutely ,  1475 ) ( absurd ,  288 ) ( abuse ,  186 ) ( abused ,  76 ) ( abusive ,  89 ) ( abysmal ,  91 ) ( academy ,  293 ) ( accent ,  458 ) ( accents ,  197 ) ( accept ,  297 ) ( acceptable ,  124 ) ( accepted ,  141 ) ( access ,  91 ) ( accident ,  299 ) ( accidentally ,  200 ) ( accompanied ,  86 ) ( accomplished ,  118 ) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count each word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "for tag, count in zip(vocab[0:25], dist[0:25]):\n",
    "    print( \"(\",tag,\", \",count,\") \", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Random Forest\n",
    "\n",
    "Using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">RandomForestClassifier</a> from sklearn, we can generate a better classifier through finding the decision tree of various random subset-results and ranking them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "forest = forest.fit( train_data_features, train[\"sentiment\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000;  Review 2000 of 25000;  Review 3000 of 25000;  Review 4000 of 25000;  Review 5000 of 25000;  Review 6000 of 25000;  Review 7000 of 25000;  Review 8000 of 25000;  Review 9000 of 25000;  Review 10000 of 25000;  Review 11000 of 25000;  Review 12000 of 25000;  Review 13000 of 25000;  Review 14000 of 25000;  Review 15000 of 25000;  Review 16000 of 25000;  Review 17000 of 25000;  Review 18000 of 25000;  Review 19000 of 25000;  Review 20000 of 25000;  Review 21000 of 25000;  Review 22000 of 25000;  Review 23000 of 25000;  Review 24000 of 25000;  Review 25000 of 25000;  "
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# Read and process test data\n",
    "test = pd.read_csv(\"./data/testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print (\"Review %d of %d; \" % ( i+1, num_reviews ), \"\", end=\"\")          \n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Bag of Word on the Test Data \n",
    "test_data_features = vectorizer.transform(clean_test_reviews).toarray()\n",
    "\n",
    "# Use random forest to do predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"2913_8\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"4396_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"395_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"10616_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"9074_9\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          1\n",
       "4   \"12128_7\"          1\n",
       "5    \"2913_8\"          0\n",
       "6    \"4396_1\"          0\n",
       "7     \"395_2\"          0\n",
       "8   \"10616_1\"          0\n",
       "9    \"9074_9\"          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"./output/Bag_of_Words_model.csv\", index=False, quoting=3 )\n",
    "output.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this being a kaggle dataset, we know that the result has a 0.84136 accuricy rate.<br/>\n",
    "![Kaggle Results](./img/kaggle_bag_of_words.png \"0.84136 accuricy rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
