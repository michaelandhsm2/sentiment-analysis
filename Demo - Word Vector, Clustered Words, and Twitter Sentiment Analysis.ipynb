{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Vector Model...\n",
      "Loading Second Word Vector Model...\n",
      "Loading Word Centroid Map...\n",
      "Loading Trained Random Forest Classifier...\n",
      "Setting up Twitter Authentication...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import _pickle as pickle\n",
    "import tweepy\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( 'output/'+ name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "print(\"Loading Word Vector Model...\")\n",
    "model = Word2Vec.load(\"output/400features_30minwords_10context_twitter\")\n",
    "\n",
    "print(\"Loading Second Word Vector Model...\")\n",
    "model2 = Word2Vec.load(\"output/400features_30minwords_10context_twitter\")\n",
    "\n",
    "print(\"Loading Word Centroid Map...\")\n",
    "word_centroid_map = load_obj(\"twitter_word_centroid_map\")\n",
    "\n",
    "print(\"Loading Trained Random Forest Classifier...\")\n",
    "load_forest = load_obj(\"twitter_forest\")\n",
    "\n",
    "print(\"Setting up Twitter Authentication...\")\n",
    "consumer_key = \"GlYCSvDgUet79gori1M5rxmMW\"\n",
    "consumer_secret = \"JRNb6FIjsSMOu6CU4QRMdJ1kMsVd7IF6g9PnKgD2qrdeva2iFY\"\n",
    "access_token = \"259205396-ZWx5lQCRzy5GMnzmNTIQzMckDqRnzjfVnoFu0VgG\"\n",
    "access_token_secret = \"eBx4oQQYHhXRgQE6cOioMSUhzpLWNLc8c2hgL4GGmG2Kd\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def process_tweet( tweet , punctuation=False):\n",
    "    \n",
    "    tweet = re.sub('@[^\\s]+','',tweet)    \n",
    "    tweet = re.sub('((www\\.[\\s]+)|(https?:/?/?[^\\s]+))','',tweet)\n",
    "    tweet = tweet.replace('RT','')\n",
    "    tweet = tweet.replace('#','')\n",
    "    \n",
    "    if punctuation:\n",
    "        tweet = tweet.replace('.','')\n",
    "        tweet = tweet.replace(',','')\n",
    "        tweet = tweet.replace('?','')\n",
    "        tweet = tweet.replace('!','')\n",
    "        \n",
    "    words = tweet.lower().split()    \n",
    "    return( words)   \n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 10)\n",
    "\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    \n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    \n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "            \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible  horrible  horrid  aweful  icky  rubbish  incredible  unpleasant  miserable  overwhelming  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in model.most_similar(\"awful\"):\n",
    "    print(item[0],\" \",end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 77\n",
      "['sister', 'brother', 'bro', 'sis', 'mama', 'wifey', 'neice']\n",
      "\n",
      "Cluster 78\n",
      "['speechless', 'blushing', 'stoned', 'embarassed', 'venting', 'panicking', 'cynical', 'ino', 'doubting', 'grinning']\n",
      "\n",
      "Cluster 79\n",
      "['av', 'ent']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(77,80):\n",
    "\n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    words = []\n",
    "    \n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "    \n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "            \n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tweets...\n",
      "Pre-allocating an Array...\n",
      "Producing Test Centroids...\n",
      "Predicting Test Sets...\n",
      "\n",
      "Prediction :\n",
      "    Positive - 74.20%,\n",
      "    Negative - 25.80%\n"
     ]
    }
   ],
   "source": [
    "query = \"Star Wars\"\n",
    "max_tweets = 500\n",
    "\n",
    "print (\"Loading Tweets...\")   \n",
    "searched_tweets = [status.text for status in tweepy.Cursor(api.search, q=query, lang=\"en\").items(max_tweets)]\n",
    "\n",
    "import numpy as np\n",
    "print (\"Pre-allocating an Array...\")   \n",
    "user_centroids = np.zeros( (max_tweets, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "    \n",
    "print (\"Producing Test Centroids...\")   \n",
    "counter = 0\n",
    "for tweet in searched_tweets:\n",
    "    user_centroids[counter] = create_bag_of_centroids( process_tweet( tweet, True ), word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "print (\"Predicting Test Sets...\")\n",
    "result = load_forest.predict(user_centroids)\n",
    "\n",
    "unique, counts = np.unique(result, return_counts=True)\n",
    "result_dict = dict(zip(unique, counts))\n",
    "\n",
    "print (\"\\nPrediction :\")\n",
    "print (\"    Positive - %.2f%%,\\n    Negative - %.2f%%\" %\n",
    "       (result_dict.get(4, 0)*100/len(result),\\\n",
    "       result_dict.get(0, 0)*100/len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "RT @MarkHarrisNYC: After 20 children were murdered in Sandy Hook, Paul Ryan said Obama was talking about it \"to distract from basically his…\n",
      "\n",
      "Positive\n",
      "RT @Rosie: i agree - paul ryan must go along with this inhumane administration \n",
      "#RESIST ALL THINGS @TRUMP https://t.co/amzEZhZZvf\n",
      "\n",
      "Positive\n",
      "RT @Rosie: i agree - paul ryan must go along with this inhumane administration \n",
      "#RESIST ALL THINGS @TRUMP https://t.co/amzEZhZZvf\n",
      "\n",
      "Negative\n",
      "Yulin Dog Meat Festival Starts June 21, 2017 - Speaker of the House Paul Ryan Stop the Tortu... https://t.co/DLuGua8bna vía @ChangeorgLatino\n",
      "\n",
      "Negative\n",
      "RT @MarkHarrisNYC: After 20 children were murdered in Sandy Hook, Paul Ryan said Obama was talking about it \"to distract from basically his…\n",
      "\n",
      "Negative\n",
      "RT @bbusa617: PAUL RYAN  Ran to Microphone to Condemn Trump Comments But Has Said NOTHING About Trump Assassination Play https://t.co/G5pRC…\n",
      "\n",
      "Negative\n",
      "RT @Kriquette01: @greenhousenyt @MikeAndy128 It is so unpopular, Mitch McConnell and Paul Ryan r in secret trying to pass it under the rada…\n",
      "\n",
      "Negative\n",
      "Although ... that Paul Ryan one https://t.co/dHSubBwtsx\n",
      "\n",
      "Negative\n",
      "RT @awkward_1110: Yulin Dog Meat Festival Starts June 21, 2017 - Speaker of the House Paul Ryan Stop the Torture ... https://t.co/fvCdDC4FQ…\n",
      "\n",
      "Positive\n",
      "RT @StefanMolyneux: Paul Ryan, Nancy Pelosi and Jake Tapper.\n",
      "\n",
      "They might as well be wearing matching jerseys since they're already on the s…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def switch(x): \n",
    "    if x == 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "formatted_result = list(map(switch, result))\n",
    "\n",
    "output = list(zip(formatted_result,searched_tweets))\n",
    "\n",
    "for item in output[0:10]:\n",
    "    print(item[0]+\"\\n\"+item[1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
