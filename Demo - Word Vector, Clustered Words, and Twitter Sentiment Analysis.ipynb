{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Vector Model...\n",
      "Loading Word Centroid Map...\n",
      "Loading Trained Random Forest Classifier...\n",
      "Setting up Twitter Authentication...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import _pickle as pickle\n",
    "import tweepy\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( 'output/'+ name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "print(\"Loading Word Vector Model...\")\n",
    "model = Word2Vec.load(\"output/400features_30minwords_10context_twitter\")\n",
    "\n",
    "print(\"Loading Word Centroid Map...\")\n",
    "word_centroid_map = load_obj(\"twitter_word_centroid_map\")\n",
    "\n",
    "print(\"Loading Trained Random Forest Classifier...\")\n",
    "load_forest = load_obj(\"twitter_forest\")\n",
    "\n",
    "print(\"Setting up Twitter Authentication...\")\n",
    "consumer_key = \"GlYCSvDgUet79gori1M5rxmMW\"\n",
    "consumer_secret = \"JRNb6FIjsSMOu6CU4QRMdJ1kMsVd7IF6g9PnKgD2qrdeva2iFY\"\n",
    "access_token = \"259205396-ZWx5lQCRzy5GMnzmNTIQzMckDqRnzjfVnoFu0VgG\"\n",
    "access_token_secret = \"eBx4oQQYHhXRgQE6cOioMSUhzpLWNLc8c2hgL4GGmG2Kd\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def process_tweet( tweet , punctuation=False):\n",
    "    \n",
    "    tweet = re.sub('@[^\\s]+','',tweet)    \n",
    "    tweet = re.sub('((www\\.[\\s]+)|(https?:/?/?[^\\s]+))','',tweet)\n",
    "    tweet = tweet.replace('RT','')\n",
    "    tweet = tweet.replace('#','')\n",
    "    \n",
    "    if punctuation:\n",
    "        tweet = tweet.replace('.','')\n",
    "        tweet = tweet.replace(',','')\n",
    "        tweet = tweet.replace('?','')\n",
    "        tweet = tweet.replace('!','')\n",
    "        \n",
    "    words = tweet.lower().split()    \n",
    "    return( words)   \n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    \n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    \n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "            \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "boy  dude  woman  guy  girl  kid  gosh  beast  chick  lady  \n",
      "king  lion  goddess  dancer  rendition  caribbean  course)  clark  mary  baker  \n",
      "terrible  horrible  horrid  aweful  icky  rubbish  incredible  unpleasant  miserable  overwhelming  \n"
     ]
    }
   ],
   "source": [
    "print( model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "\n",
    "for item in model.most_similar(\"man\"):\n",
    "    print(item[0],\" \",end=\"\")\n",
    "print()\n",
    "\n",
    "for item in model.most_similar(\"queen\"):\n",
    "    print(item[0],\" \",end=\"\")\n",
    "print()\n",
    "\n",
    "for item in model.most_similar(\"awful\"):\n",
    "    print(item[0],\" \",end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 8\n",
      "['google', 'web', 'via', 'search', 'software', 'network', 'main', 'testing', 'data', 'mode', 'flash', 'speed', 'itself', 'feature', 'technology', 'user', 'photoshop', 'bing', 'wordpress', 'maintenance', 'ie', 'tool', '-&gt;', 'installing', 'virtual', 'linux', 'limited', 'crashes', 'loading', 'auto', 'engine', '&lt;', 'dev', 'map', 'setup', 'standard', 'correctly', 'basic', 'php', 'format', 'adobe', 'ui', 'css', 'filter', 'plugin', 'paste', 'solved', 'successfully', 'font', 'developer', 'connecting', 'interface', 'maps', 'wp', 'tab', 'html', 'alpha', 'platform', 'upgrading', 'database', 'python', 'developing', 'overload', '64', 'tabs', 'internal', 'meanwhile', 'unite', 'adds', 'buggy', 'drupal', 'explorer', 'reliable', 'db', 'stable', 'unavailable', 'excel', 'usage', 'downtime', 'jquery', '==', 'syncing', 'ip', 'activation', 'command', 'provider', 'ie6', 'sql', 'bandwidth', '=&gt;', 'template', 'installation', 'cs4', 'controls', 'corrupted', 'sharepoint', 'javascript', 'cache', 'rendering', 'joomla', 'rails', 'proxy', 'managing', 'visible', 'pdf', 'reinstalling', 'manually', 'banking', 'flex', 'dns', 'wolfram', 'bookmarks', 'firewall', 'generic', '904', 'illustrator', 'vmware', 'mesh', 'enterprise', 'converting', 'gateway', 'cms', 'ie8', '2003', 'builder', 'mozilla', 'fatal', 'operating', 'analytics', 'xml', 'ftp', 'restoring', 'corrupt', 'debugging', 'functionality', 'framework', 'silverlight', 'merge', 'vpn', 'borked', 'communications', 'speeds', \"pc's\", 'directory', 'clicks', 'ide', 'uploads', 'gnome', 'render', 'spaz', 'translator', 'ranking', 'backups', 'bingcom', 'timeout', 'dongle', 'toolbar', 'ports', \"google's\", 'ajax', 'solving', 'perl', 'conversion', 'ppt', 'parallels', 'builds', 'aspnet', 'mobileme', 'handles', 'provides', 'embed', 'keyword', 'compatibility', 'wolframalpha', 'monitoring', 'widgets', 'pidgin', 'gui', 'config', 'dreamhost', 'logs', 'stored', 'generator', 'reinstalled', 'export', 'adsense', 'integrated', 'connectivity', 'multimedia', 'plumbing', 'minimize', 'godaddy', 'restricted', 'integrate', 'formatting', 'dropbox', 'iphoto', 'twibble', 'compile', 'implemented', \"site's\", '1057', 'censorship', 'partition', 'adwords', 'kernel', 'templates', 'sandbox', 'installs', 'tweak', 'mapping', 'indesign', 'ie7', 'migration', 'kde', 'providers', 'navigation', 'embedded', 'displays', 'signup', 'latitude', 'engineers', 'capabilities', 'm$', 'usability', 'cs3', 'io', 'r2', 'shortening', 'detect']\n",
      "\n",
      "Cluster 9\n",
      "['patti']\n",
      "\n",
      "Cluster 10\n",
      "['pho', 'foood']\n",
      "\n",
      "Cluster 11\n",
      "['sits']\n",
      "\n",
      "Cluster 12\n",
      "['thk']\n",
      "\n",
      "Cluster 13\n",
      "['registration']\n",
      "\n",
      "Cluster 14\n",
      "['like', 'fancy', 'familiar', 'promising', 'appealing', 'importantly', 'closely', 'suitable', 'intriguing', 'carefully', 'merrier']\n",
      "\n",
      "Cluster 15\n",
      "['de', 'el', 'que', 'ï¿½', 'eu', 'en', 'se', 'q', 'un', 'lo', 'com', 'dia', 'hai', 'es', 'tu', 'bom', 'ke', 'por', 'je', 'mai', 'cu', 'ja', 'nada', 'ti', 'gi', 'nu', 'mas', 'ich', 'ou', 'das', 'ver', 'ar', 'para', 'vi', 'nao', 'und', 'vou', 'pas', 'boa', 'ce', 'som', 'har', 'meu', 've', 'tb', 'tot', 'amo', 'ã©', 'ir', 'mal', 'mein', 'mais', 'ã', 'como', 'ra', 'dum', 'foto', 'triste', 'sur', 'pra', 'ani', 'nun', 'nahi', 'amor', 'nem', 'tem', 'bem', 'sono', 'tarde', 'gusta', 'vc', 'tengo', 'sou', 'nã£o', 'una', 'acho', 'vai', 'dir', 'dar', 'esta', 'hoy', 'deja', 'ao', 'ser', 'eso', 'bien', 'hein', 'fotos', 'queria', 'quiero', 'siento', 'ele', 'quero', 'asi', 'sunt', 'muito', 'voy', 'viendo', 'hj', 'za', 'jã¡', 'hoje']\n",
      "\n",
      "Cluster 16\n",
      "['lmaoo']\n",
      "\n",
      "Cluster 17\n",
      "['67']\n",
      "\n",
      "Cluster 18\n",
      "['usa']\n",
      "\n",
      "Cluster 19\n",
      "['timberlake']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(8,20):\n",
    "\n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    words = []\n",
    "    \n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "    \n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "            \n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tweets...\n",
      "Pre-allocating an Array...\n",
      "Producing Test Centroids...\n",
      "Predicting Test Sets...\n",
      "\n",
      "Prediction :\n",
      "    Positive - 52.20%,\n",
      "    Negative - 47.80%\n"
     ]
    }
   ],
   "source": [
    "query = \"Paul Ryan\"\n",
    "max_tweets = 500\n",
    "\n",
    "print (\"Loading Tweets...\")   \n",
    "searched_tweets = [status.text for status in tweepy.Cursor(api.search, q=query, lang=\"en\").items(max_tweets)]\n",
    "\n",
    "import numpy as np\n",
    "print (\"Pre-allocating an Array...\")   \n",
    "user_centroids = np.zeros( (max_tweets, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "    \n",
    "print (\"Producing Test Centroids...\")   \n",
    "counter = 0\n",
    "for tweet in searched_tweets:\n",
    "    user_centroids[counter] = create_bag_of_centroids( process_tweet( tweet, True ), word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "print (\"Predicting Test Sets...\")\n",
    "result = load_forest.predict(user_centroids)\n",
    "\n",
    "unique, counts = np.unique(result, return_counts=True)\n",
    "result_dict = dict(zip(unique, counts))\n",
    "\n",
    "print (\"\\nPrediction :\")\n",
    "print (\"    Positive - %.2f%%,\\n    Negative - %.2f%%\" %\n",
    "       (result_dict.get(4, 0)*100/len(result),\\\n",
    "       result_dict.get(0, 0)*100/len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "RT @BobbiFotsch: #Yulin dog meat festival starts June21. 10K #dogs will be tortured and killed. Call SOH Paul Ryan 202-225-3031. Ask him to…\n",
      "\n",
      "Negative\n",
      "RT @funder: Trump goes to Wisconsin &amp; they're already losing hundreds of jobs. Sounds about right.\n",
      "\n",
      "Support @DavidYankovich who's gonna bea…\n",
      "\n",
      "Negative\n",
      "RT @funder: Trump goes to Wisconsin &amp; they're already losing hundreds of jobs. Sounds about right.\n",
      "\n",
      "Support @DavidYankovich who's gonna bea…\n",
      "\n",
      "Negative\n",
      "RT @funder: Trump goes to Wisconsin &amp; they're already losing hundreds of jobs. Sounds about right.\n",
      "\n",
      "Support @DavidYankovich who's gonna bea…\n",
      "\n",
      "Positive\n",
      "Paul Ryan's org rakes in big dollars, feat @UWMadison expert Eleanor Powell https://t.co/SBDOTojWWQ… https://t.co/uPJP2OMi8b\n",
      "\n",
      "Positive\n",
      "RT @MailSport: Ryan Giggs: Man United fans should be excited to see Paul Pogba in action next season  https://t.co/cLLs0I1L5F https://t.co/…\n",
      "\n",
      "Positive\n",
      "RT @JohnFugelsang: Hey Paul Ryan!\n",
      "See the conman w/the scam college?\n",
      "Who mocked a disabled guy?\n",
      "&amp; you called textbook racist?\n",
      "He just said…\n",
      "\n",
      "Negative\n",
      "RT @funder: Trump goes to Wisconsin &amp; they're already losing hundreds of jobs. Sounds about right.\n",
      "\n",
      "Support @DavidYankovich who's gonna bea…\n",
      "\n",
      "Positive\n",
      "@juliettekayyem I think Paul Ryan, Lankford, Graham (GOP) are drawing (red) line. That Trump shouldn't fire Mueller… https://t.co/rkBUb5TQC7\n",
      "\n",
      "Negative\n",
      "RT @funder: Trump goes to Wisconsin &amp; they're already losing hundreds of jobs. Sounds about right.\n",
      "\n",
      "Support @DavidYankovich who's gonna bea…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def switch(x): \n",
    "    if x == 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "formatted_result = list(map(switch, result))\n",
    "\n",
    "output = list(zip(formatted_result,searched_tweets))\n",
    "\n",
    "for item in output[0:10]:\n",
    "    print(item[0]+\"\\n\"+item[1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
