{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Vector Model...\n",
      "Loading Word Centroid Map...\n",
      "Loading Trained Random Forest Classifier...\n",
      "Setting up Twitter Authentication...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import _pickle as pickle\n",
    "import tweepy\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( 'output/'+ name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "print(\"Loading Word Vector Model...\")\n",
    "model = Word2Vec.load(\"output/400features_30minwords_10context_twitter\")\n",
    "\n",
    "print(\"Loading Second Word Vector Model...\")\n",
    "model2 = Word2Vec.load(\"output/400features_30minwords_10context_twitter\")\n",
    "\n",
    "print(\"Loading Word Centroid Map...\")\n",
    "word_centroid_map = load_obj(\"twitter_word_centroid_map\")\n",
    "\n",
    "print(\"Loading Trained Random Forest Classifier...\")\n",
    "load_forest = load_obj(\"twitter_forest\")\n",
    "\n",
    "print(\"Setting up Twitter Authentication...\")\n",
    "consumer_key = \"GlYCSvDgUet79gori1M5rxmMW\"\n",
    "consumer_secret = \"JRNb6FIjsSMOu6CU4QRMdJ1kMsVd7IF6g9PnKgD2qrdeva2iFY\"\n",
    "access_token = \"259205396-ZWx5lQCRzy5GMnzmNTIQzMckDqRnzjfVnoFu0VgG\"\n",
    "access_token_secret = \"eBx4oQQYHhXRgQE6cOioMSUhzpLWNLc8c2hgL4GGmG2Kd\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def process_tweet( tweet , punctuation=False):\n",
    "    \n",
    "    tweet = re.sub('@[^\\s]+','',tweet)    \n",
    "    tweet = re.sub('((www\\.[\\s]+)|(https?:/?/?[^\\s]+))','',tweet)\n",
    "    tweet = tweet.replace('RT','')\n",
    "    tweet = tweet.replace('#','')\n",
    "    \n",
    "    if punctuation:\n",
    "        tweet = tweet.replace('.','')\n",
    "        tweet = tweet.replace(',','')\n",
    "        tweet = tweet.replace('?','')\n",
    "        tweet = tweet.replace('!','')\n",
    "        \n",
    "    words = tweet.lower().split()    \n",
    "    return( words)   \n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 10)\n",
    "\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    \n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    \n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "            \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "boy  dude  woman  guy  girl  kid  gosh  beast  chick  lady  \n",
      "king  lion  goddess  dancer  rendition  caribbean  course)  clark  mary  baker  \n",
      "terrible  horrible  horrid  aweful  icky  rubbish  incredible  unpleasant  miserable  overwhelming  \n"
     ]
    }
   ],
   "source": [
    "print( model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "\n",
    "for item in model.most_similar(\"man\"):\n",
    "    print(item[0],\" \",end=\"\")\n",
    "print()\n",
    "\n",
    "for item in model.most_similar(\"queen\"):\n",
    "    print(item[0],\" \",end=\"\")\n",
    "print()\n",
    "\n",
    "for item in model.most_similar(\"awful\"):\n",
    "    print(item[0],\" \",end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['supporting', 'promoting', 'amongst', 'interviewing']\n",
      "\n",
      "Cluster 1\n",
      "[\"baby's\", \"husband's\", 'fir', \"boyfriend's\", 'baptism', \"gf's\", 'crate', 'nans', 'arrangements', 'inlaws', \"grandmother's\", 'grandmothers', 'granddaughter']\n",
      "\n",
      "Cluster 2\n",
      "['spore']\n",
      "\n",
      "Cluster 3\n",
      "['burning', 'watering', 'weeding', 'peeled', 'scrubbing']\n",
      "\n",
      "Cluster 4\n",
      "['windsor', 'lancaster', 'courts', 'daytona', 'ballpark']\n",
      "\n",
      "Cluster 5\n",
      "[':0', '*high', 'owell']\n",
      "\n",
      "Cluster 6\n",
      "['phone', 'computer', 'bb', 'blackberry', 'cell', 'battery', 'comp', 'phones', 'charger', 'contacts', 'headphones', 'fone', 'batteries', 'sidekick', 'cellphone', \"phone's\", 'earphones', 'crackberry', 'ipods', 'splinter', 'puter', 'i-pod']\n",
      "\n",
      "Cluster 7\n",
      "['dim', 'snickers', 'granola', 'twix', 'pastry', 'gourmet', 'boxed', 'choclate', 'mochi']\n",
      "\n",
      "Cluster 8\n",
      "['me&quot;', \"'\", 'seek', \"me'\", 'people&quot;', '&quot;fly', 'dance&quot;']\n",
      "\n",
      "Cluster 9\n",
      "['brady', 'musicals', 'rugrats', 'hogs']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "\n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    words = []\n",
    "    \n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "    \n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "            \n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tweets...\n",
      "Pre-allocating an Array...\n",
      "Producing Test Centroids...\n",
      "Predicting Test Sets...\n",
      "\n",
      "Prediction :\n",
      "    Positive - 68.80%,\n",
      "    Negative - 31.20%\n"
     ]
    }
   ],
   "source": [
    "query = \"Paul Ryan\"\n",
    "max_tweets = 500\n",
    "\n",
    "print (\"Loading Tweets...\")   \n",
    "searched_tweets = [status.text for status in tweepy.Cursor(api.search, q=query, lang=\"en\").items(max_tweets)]\n",
    "\n",
    "import numpy as np\n",
    "print (\"Pre-allocating an Array...\")   \n",
    "user_centroids = np.zeros( (max_tweets, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "    \n",
    "print (\"Producing Test Centroids...\")   \n",
    "counter = 0\n",
    "for tweet in searched_tweets:\n",
    "    user_centroids[counter] = create_bag_of_centroids( process_tweet( tweet, True ), word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "print (\"Predicting Test Sets...\")\n",
    "result = load_forest.predict(user_centroids)\n",
    "\n",
    "unique, counts = np.unique(result, return_counts=True)\n",
    "result_dict = dict(zip(unique, counts))\n",
    "\n",
    "print (\"\\nPrediction :\")\n",
    "print (\"    Positive - %.2f%%,\\n    Negative - %.2f%%\" %\n",
    "       (result_dict.get(4, 0)*100/len(result),\\\n",
    "       result_dict.get(0, 0)*100/len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "RT @TemmiV: Paul Ryan got 20 million donation to his campaign. TWENTY MILLION. https://t.co/gjyLIyaOF7\n",
      "\n",
      "Positive\n",
      "RT @StefanMolyneux: Paul Ryan, Nancy Pelosi and Jake Tapper.\n",
      "\n",
      "They might as well be wearing matching jerseys since they're already on the s…\n",
      "\n",
      "Negative\n",
      "RT @BriInWI: Well, if anyone was wondering about the price of Paul Ryan's dignity and spine, it's $20 million. https://t.co/QhrZzqVKiS\n",
      "\n",
      "Positive\n",
      "RT @emigre80: Just so you all know, Paul Ryan, the Speaker of the House, was willing to support Trump in exchange for $20,000,000. https://…\n",
      "\n",
      "Positive\n",
      "RT @AnthonyCumia: Paul Ryan gets ready to throw out the first bitch.\n",
      "#CongressionalBaseballGame https://t.co/eutDiIhiz4\n",
      "\n",
      "Positive\n",
      "RT @StefanMolyneux: Paul Ryan, Nancy Pelosi and Jake Tapper.\n",
      "\n",
      "They might as well be wearing matching jerseys since they're already on the s…\n",
      "\n",
      "Positive\n",
      "RT @mterr337: Mike Pence Lawyers Up: https://t.co/yQNHj1uRSe As Paul Ryan counts the days to his Presidency.\n",
      "\n",
      "Positive\n",
      "RT @funder: Doc: Paul Ryan got $20,000,000 from 2 of Trump's biggest campaign funders in August 2016-for his super PAC\n",
      "\n",
      "#TrumpLeaks #TrumpR…\n",
      "\n",
      "Positive\n",
      "RT @Isabellarowling: Oh, so THAT'S why Paul Ryan wags his tail when Trump pulls on his leash! https://t.co/8aKYJkFHsf\n",
      "\n",
      "Positive\n",
      "RT @emigre80: Just so you all know, Paul Ryan, the Speaker of the House, was willing to support Trump in exchange for $20,000,000. https://…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def switch(x): \n",
    "    if x == 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "formatted_result = list(map(switch, result))\n",
    "\n",
    "output = list(zip(formatted_result,searched_tweets))\n",
    "\n",
    "for item in output[0:10]:\n",
    "    print(item[0]+\"\\n\"+item[1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
